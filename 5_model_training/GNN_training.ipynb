{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18f95df",
   "metadata": {},
   "source": [
    "Create and train different GNNs using Spektral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7fc53",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "28364b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, pickle\n",
    "import spektral\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import layers\n",
    "from spektral.data import Dataset\n",
    "from spektral.data.loaders import DisjointLoader\n",
    "from spektral.layers import GCNConv, GATConv, ECCConv, GraphSageConv, GINConv, GlobalSumPool, GlobalAvgPool, GlobalMaxPool, GeneralConv\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97f6cd",
   "metadata": {},
   "source": [
    "## 2. Reproducibility Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1c90014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 55\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"   # enforce deterministic ops (GPU too)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load Custom Dataset to be able to load pickle file\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, graph_list, **kwargs):\n",
    "        self.graph_list = graph_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        return self.graph_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755e4c7",
   "metadata": {},
   "source": [
    "## 3. Load Dataset and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66706a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 30) (2, 12) (12, 11) (512,) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../4_gnn_data_pipeline/dataset_deepchem.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)   # already a Spektral Dataset object\n",
    "\n",
    "graph = dataset[0]\n",
    "\n",
    "# Looking at shape of features in the dataset\n",
    "x = graph.x         # node features\n",
    "a = graph.a         # adjacency / edge indices\n",
    "e = graph.e         # edge features\n",
    "y = graph.y         # target(s)\n",
    "g = graph.globals   # global features\n",
    "\n",
    "print(x.shape, a.shape, e.shape if e is not None else None, y.shape, g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9dcf9",
   "metadata": {},
   "source": [
    "## 4. Define GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "43e1452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_EdgeModel(Model):\n",
    "    def __init__(self, n_node_features, n_edge_features, n_globals=0, n_targets=1, hidden=64, dropout=0.0, pool=\"avg\"):\n",
    "        super().__init__()\n",
    "        self.n_node_features = n_node_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self.n_globals = n_globals\n",
    "        self.n_targets = n_targets\n",
    "\n",
    "        # Edge-aware convolutions\n",
    "        self.conv1 = ECCConv(hidden, activation=\"relu\")\n",
    "        self.conv2 = ECCConv(hidden, activation=\"relu\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = {\"sum\": GlobalSumPool(),\n",
    "                     \"avg\": GlobalAvgPool(),\n",
    "                     \"max\": GlobalMaxPool()}[pool]\n",
    "\n",
    "        # Dense head\n",
    "        self.fc = layers.Dense(hidden, activation=\"relu\")\n",
    "        self.out = layers.Dense(n_targets, activation=\"linear\")  # regression output\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: tuple from DisjointLoader:\n",
    "            x: node features (num_nodes, n_node_features)\n",
    "            a: adjacency matrix, SparseTensor (num_edges, 2)\n",
    "            e: edge features (num_edges, n_edge_features)\n",
    "            i: batch index vector (num_nodes,) mapping node -> graph\n",
    "            g: global features (num_graphs_in_batch, n_globals) or None\n",
    "        \"\"\"\n",
    "        x, a, i, e, g = inputs\n",
    "\n",
    "        # Ensure edge features are 2D\n",
    "        if e is not None and len(e.shape) == 1:\n",
    "            e = tf.expand_dims(e, axis=-1)\n",
    "\n",
    "        # ECCConv expects SparseTensor adjacency and edge features\n",
    "        x = self.conv1([x, a, e])\n",
    "        x = self.conv2([x, a, e])\n",
    "\n",
    "        # Dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Graph pooling\n",
    "        x = self.pool([x, i])\n",
    "\n",
    "        # Concatenate global features if present\n",
    "        if g is not None and self.n_globals > 0:\n",
    "            x = tf.concat([x, g], axis=-1)\n",
    "\n",
    "        # Dense head\n",
    "        x = self.fc(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "150b5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't take in edges; have to add a custom layer for that later\n",
    "class GATModel(Model):\n",
    "    def __init__(self, n_node_features, n_edge_features, n_globals, n_targets=1, hidden=64, dropout=0.0, heads=4, pool=\"avg\"):\n",
    "        super().__init__()\n",
    "        self.n_node_features = n_node_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self.n_globals = n_globals\n",
    "\n",
    "        self.conv1 = GATConv(hidden, attn_heads=heads, concat_heads=True, activation=\"elu\")\n",
    "        self.conv2 = GATConv(hidden, attn_heads=1, concat_heads=False, activation=\"elu\")\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.pool = {\"sum\": GlobalSumPool(),\n",
    "                     \"avg\": GlobalAvgPool(),\n",
    "                     \"max\": GlobalMaxPool()}[pool]\n",
    "        self.fc = Dense(hidden, activation=\"relu\")\n",
    "        self.out = Dense(n_targets, activation=\"linear\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, a, i, e, g = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.pool([x, i])\n",
    "        x = tf.concat([x, g], axis=-1)\n",
    "        x = self.fc(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d345f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also doesn't use edges\n",
    "class GraphSAGEModel(Model):\n",
    "    def __init__(self, n_node_features, n_edge_features, n_globals, n_targets=1, hidden=64, dropout=0.0, pool=\"avg\"):\n",
    "        super().__init__()\n",
    "        self.n_node_features = n_node_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self.n_globals = n_globals\n",
    "\n",
    "        self.conv1 = GraphSageConv(hidden, activation=\"relu\")\n",
    "        self.conv2 = GraphSageConv(hidden, activation=\"relu\")\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.pool = {\"sum\": GlobalSumPool(),\n",
    "                     \"avg\": GlobalAvgPool(),\n",
    "                     \"max\": GlobalMaxPool()}[pool]\n",
    "        self.fc = Dense(hidden, activation=\"relu\")\n",
    "        self.out = Dense(n_targets, activation=\"linear\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, a, i, e, g = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.pool([x, i])\n",
    "        x = tf.concat([x, g], axis=-1)\n",
    "        x = self.fc(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dfcbb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also doesn't use edges\n",
    "class GINModel(Model):\n",
    "    def __init__(self, n_node_features, n_edge_features, n_globals, n_targets=1, hidden=64, dropout=0.0, pool=\"avg\"):\n",
    "        super().__init__()\n",
    "        self.n_node_features = n_node_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self.n_globals = n_globals\n",
    "        # Each GINConv requires an MLP\n",
    "        mlp1 = tf.keras.Sequential([Dense(hidden, activation=\"relu\"), Dense(hidden, activation=\"relu\")])\n",
    "        mlp2 = tf.keras.Sequential([Dense(hidden, activation=\"relu\"), Dense(hidden, activation=\"relu\")])\n",
    "        self.conv1 = GINConv(mlp1)\n",
    "        self.conv2 = GINConv(mlp2)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.pool = {\"sum\": GlobalSumPool(),\n",
    "                     \"avg\": GlobalAvgPool(),\n",
    "                     \"max\": GlobalMaxPool()}[pool]\n",
    "        self.fc = Dense(hidden, activation=\"relu\")\n",
    "        self.out = Dense(n_targets, activation=\"linear\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, a, i, e, g = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.pool([x, i])\n",
    "        x = tf.concat([x, g], axis=-1)\n",
    "        x = self.fc(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0a74a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNModel(Model):\n",
    "    def __init__(self, n_node_features, n_edge_features, n_globals, n_targets=1, hidden=64, dropout=0.0, pool=\"avg\"):\n",
    "        super().__init__()\n",
    "        self.n_node_features = n_node_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self.n_globals = n_globals\n",
    "\n",
    "        self.conv1 = MPNNConv(hidden, activation=\"relu\")\n",
    "        self.conv2 = MPNNConv(hidden, activation=\"relu\")\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.pool = {\"sum\": GlobalSumPool(),\n",
    "                     \"avg\": GlobalAvgPool(),\n",
    "                     \"max\": GlobalMaxPool()}[pool]\n",
    "        self.fc = Dense(hidden, activation=\"relu\")\n",
    "        self.out = Dense(n_targets, activation=\"linear\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, a, i, e, g = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        x = self.conv2([x, a, e])\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.pool([x, i])\n",
    "        x = tf.concat([x, g], axis=-1)\n",
    "        x = self.fc(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01055d82",
   "metadata": {},
   "source": [
    "## 4. Evalution Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "278c7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14827bef",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation and hyperparameter sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "560ad03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model factory to make a fresh model for each kfold sweep to prevent biased weights\n",
    "\n",
    "def model_factory(model_name,\n",
    "                  n_node_features,\n",
    "                  n_edge_features,\n",
    "                  n_globals,\n",
    "                  n_targets=1,\n",
    "                  hidden=64,\n",
    "                  dropout=0.0,\n",
    "                  heads=4,\n",
    "                  pool=\"avg\"):\n",
    "\n",
    "    model_name = model_name.lower()\n",
    "\n",
    "    if model_name == \"gcn\":\n",
    "        return GCNModel(n_node_features=n_node_features,\n",
    "                        n_edge_features=n_edge_features,\n",
    "                        n_globals=n_globals,\n",
    "                        n_targets=n_targets,\n",
    "                        hidden=hidden,\n",
    "                        dropout=dropout,\n",
    "                        pool=pool)\n",
    "    elif model_name == \"gat\":\n",
    "        return GATModel(n_node_features=n_node_features,\n",
    "                        n_edge_features=n_edge_features,\n",
    "                        n_globals=n_globals,\n",
    "                        n_targets=n_targets,\n",
    "                        hidden=hidden,\n",
    "                        dropout=dropout,\n",
    "                        pool=pool)\n",
    "    elif model_name == \"graphsage\":\n",
    "        return GraphSAGEModel(n_node_features=n_node_features,\n",
    "                              n_edge_features=n_edge_features,\n",
    "                              n_globals=n_globals,\n",
    "                              n_targets=n_targets,\n",
    "                              hidden=hidden,\n",
    "                              dropout=dropout,\n",
    "                              pool=pool)\n",
    "    elif model_name == \"gin\":\n",
    "        return GINModel(n_node_features=n_node_features,\n",
    "                        n_edge_features=n_edge_features,\n",
    "                        n_globals=n_globals,\n",
    "                        n_targets=n_targets,\n",
    "                        hidden=hidden,\n",
    "                        dropout=dropout,\n",
    "                        pool=pool)\n",
    "    elif model_name == \"mpnn\":\n",
    "        return MPNNModel(n_node_features=n_node_features,\n",
    "                         n_edge_features=n_edge_features,\n",
    "                         n_globals=n_globals,\n",
    "                         n_targets=n_targets,\n",
    "                         hidden=hidden,\n",
    "                         dropout=dropout,\n",
    "                         pool=pool)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1f7c5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap DisjointLoader to include g so we don't have issues with tensor dimensions\n",
    "\n",
    "class DisjointLoaderWithGlobals(DisjointLoader):\n",
    "    def collate(self, batch):\n",
    "        # Call the usual disjoint collate\n",
    "        inputs, target = super().collate(batch)\n",
    "\n",
    "        # Extract globals separately\n",
    "        g = [graph.globals for graph in batch]\n",
    "        g = tf.convert_to_tensor(g, dtype=tf.float32)\n",
    "\n",
    "        # Return inputs + g\n",
    "        x, a, e, i = inputs\n",
    "        return (x, a, e, i, g), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "26f61b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 11)\n",
      "\n",
      "Starting hyperparameter sweep for GCN\n",
      "Node features: (490, 30)\n",
      "Edge indices (sparse): SparseTensor(indices=tf.Tensor(\n",
      "[[   0    0]\n",
      " [   0    3]\n",
      " [   0    4]\n",
      " ...\n",
      " [  63 1085]\n",
      " [  63 1086]\n",
      " [  63 1087]], shape=(2080, 2), dtype=int64), values=tf.Tensor([ 1.  7.  7. ... 18. 14. 18.], shape=(2080,), dtype=float32), dense_shape=tf.Tensor([  64 1088], shape=(2,), dtype=int64))\n",
      "Edge features: (490,)\n",
      "Batch index: (1088, 11)\n",
      "Global features: (32, 1, 1)\n",
      "Targets: (32, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MIchele Myong\\radical_ions_optical_spectra\\deepchem_env\\Lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'CustomDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n",
      "c:\\Users\\MIchele Myong\\radical_ions_optical_spectra\\deepchem_env\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1484: UserWarning: Layer 'gcn_model_99' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling ECCConv.call().\n",
      "\n",
      "\u001b[1mDimensions must be equal, but are 2080 and 1088 for '{{node ecc_conv_198_1/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"...ab,...abc->...ac\"](ecc_conv_198_1/GatherV2, ecc_conv_198_1/Reshape)' with input shapes: [2080,30], [1088,30,64].\u001b[0m\n",
      "\n",
      "Arguments received by ECCConv.call():\n",
      "  â€¢ inputs=['tf.Tensor(shape=(490, 30), dtype=float32)', 'tf.Tensor(shape=(64, 1088), dtype=float32)', 'tf.Tensor(shape=(1088, 11), dtype=float32)']\n",
      "  â€¢ mask=['None', 'None', 'None']''\n",
      "  warnings.warn(\n",
      "c:\\Users\\MIchele Myong\\radical_ions_optical_spectra\\deepchem_env\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'gcn_model_99', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling ECCConv.call().\n\n\u001b[1m{{function_node __wrapped____MklEinsum_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected dimension 2080 at axis 0 of the input shaped [1088,30,64] but got dimension 1088 [Op:Einsum] name: \u001b[0m\n\nArguments received by ECCConv.call():\n  â€¢ inputs=['tf.Tensor(shape=(490, 30), dtype=float32)', 'tf.Tensor(shape=(64, 1088), dtype=float32)', 'tf.Tensor(shape=(1088, 11), dtype=float32)']\n  â€¢ mask=['None', 'None', 'None']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[301]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     loss = loss_fn(y, y_pred)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Backwards pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MIchele Myong\\radical_ions_optical_spectra\\deepchem_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[279]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mGCNModel.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m     36\u001b[39m x = tf.convert_to_tensor(x, dtype=tf.float32)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Edge indices: keep as int64, shape (2, num_edges)\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do NOT convert to float or add extra dimensions\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m#a = tf.convert_to_tensor(a, dtype=tf.int64)\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Edge-aware convolutions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv2([x, a, e])\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Dropout\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MIchele Myong\\radical_ions_optical_spectra\\deepchem_env\\Lib\\site-packages\\spektral\\layers\\convolutional\\conv.py:74\u001b[39m, in \u001b[36mcheck_dtypes_decorator.<locals>._inner_check_dtypes\u001b[39m\u001b[34m(inputs, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(call)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_inner_check_dtypes\u001b[39m(inputs, **kwargs):\n\u001b[32m     73\u001b[39m     inputs = check_dtypes(inputs)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MIchele Myong\\radical_ions_optical_spectra\\deepchem_env\\Lib\\site-packages\\spektral\\layers\\convolutional\\ecc_conv.py:175\u001b[39m, in \u001b[36mECCConv.call\u001b[39m\u001b[34m(self, inputs, mask)\u001b[39m\n\u001b[32m    173\u001b[39m     index_sources = a.indices[:, \u001b[32m0\u001b[39m]\n\u001b[32m    174\u001b[39m     messages = tf.gather(x, index_sources, axis=-\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     messages = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m...ab,...abc->...ac\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     output = ops.scatter_sum(messages, index_targets, N)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Exception encountered when calling ECCConv.call().\n\n\u001b[1m{{function_node __wrapped____MklEinsum_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected dimension 2080 at axis 0 of the input shaped [1088,30,64] but got dimension 1088 [Op:Einsum] name: \u001b[0m\n\nArguments received by ECCConv.call():\n  â€¢ inputs=['tf.Tensor(shape=(490, 30), dtype=float32)', 'tf.Tensor(shape=(64, 1088), dtype=float32)', 'tf.Tensor(shape=(1088, 11), dtype=float32)']\n  â€¢ mask=['None', 'None', 'None']"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "hidden_options = [64, 128, 256]\n",
    "dropout_options = [0.0, 0.2, 0.4]\n",
    "lr_options = [1e-3, 5e-4, 1e-4]\n",
    "pool_options = [\"sum\", \"avg\", \"max\"]\n",
    "\n",
    "spectrum_length = 100 #Want to predict the absorption spectrum with this number of points\n",
    "\n",
    "n_hidden = 64\n",
    "n_outputs = spectrum_length\n",
    "\n",
    "model_names = [\"GCN\", \"GAT\", \"GraphSAGE\", \"GIN\", \"MPNN\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 10-fold CV\n",
    "# -----------------------------\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "results = []\n",
    "# ------------------------------\n",
    "# Input feature sizes \n",
    "# ------------------------------\n",
    "n_node_features = graph.x.shape[-1] # or dataset.node_features.shape[-1] \n",
    "n_edge_indices = graph.a.shape[-1]\n",
    "n_edge_features = graph.e.shape # or 0 if no edge features \n",
    "n_globals = graph.globals.shape # or 0 if no global features \n",
    "n_targets = 1 # regression target\n",
    "\n",
    "print(n_edge_features)\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nStarting hyperparameter sweep for {model_name}\")\n",
    "    \n",
    "    for hidden in hidden_options:\n",
    "        for dropout in dropout_options:\n",
    "            for lr in lr_options:\n",
    "                for pool in pool_options:\n",
    "                    fold_mae = []\n",
    "                    \n",
    "                    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset), 1):\n",
    "                        # Split dataset\n",
    "                        train_dataset = dataset[train_idx.tolist()]\n",
    "                        val_dataset = dataset[val_idx.tolist()]\n",
    "                        \n",
    "                        train_loader = DisjointLoaderWithGlobals(train_dataset, batch_size=32, shuffle=True, node_level=False)\n",
    "                        val_loader = DisjointLoaderWithGlobals(val_dataset, batch_size=32, shuffle=False, node_level=False)\n",
    "\n",
    "                        # Build model\n",
    "                        model = model_factory(\n",
    "                            model_name,\n",
    "                            n_node_features, \n",
    "                            n_edge_features, \n",
    "                            n_globals, \n",
    "                            n_targets,\n",
    "                            hidden,\n",
    "                            dropout,\n",
    "                            pool\n",
    "                        )\n",
    "                        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "                        \n",
    "                        # Training per epoch\n",
    "                        epochs = 50\n",
    "                        patience = 10\n",
    "                        best_val_mae = np.inf\n",
    "                        wait = 0\n",
    "                        best_weights = None\n",
    "                        \n",
    "                        for epoch in range(epochs):\n",
    "                            # ---- Training ----\n",
    "                            for batch in train_loader:  \n",
    "                                # Looking at shape of features in the dataset\n",
    "                                inputs, y = batch\n",
    "                                x, a, i, e, g = inputs\n",
    "                                print(\"Node features:\", x.shape)\n",
    "                                print(\"Edge indices (sparse):\", a)\n",
    "                                print(\"Edge features:\", e.shape if e is not None else None)\n",
    "                                print(\"Batch index:\", i.shape)\n",
    "                                print(\"Global features:\", g.shape if g is not None else None)\n",
    "                                print(\"Targets:\", y.shape)\n",
    "\n",
    "                                # Forward pass\n",
    "                                with tf.GradientTape() as tape:\n",
    "                                    y_pred = model([x, a, i, e, g], training=True)\n",
    "                                    loss = loss_fn(y, y_pred)\n",
    "                                # Backwards pass\n",
    "                                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "                            \n",
    "                            # ---- Validation ----\n",
    "                            val_maes = []\n",
    "                            for batch in val_loader:\n",
    "                                inputs, y = batch\n",
    "                                x, a, i, e, g = inputs\n",
    "                                y_pred = model([x, a, i, e, g], training=False)\n",
    "                                val_maes.append(tf.reduce_mean(tf.keras.losses.mean_absolute_error(y, y_pred)).numpy())\n",
    "                            \n",
    "                            val_mae = np.mean(val_maes)\n",
    "                            \n",
    "                            if val_mae < best_val_mae:\n",
    "                                best_val_mae = val_mae\n",
    "                                best_weights = model.get_weights()\n",
    "                                wait = 0\n",
    "                            else:\n",
    "                                wait += 1\n",
    "                            \n",
    "                            if wait >= patience:\n",
    "                                break\n",
    "                        \n",
    "                        # Restore best weights\n",
    "                        model.set_weights(best_weights)\n",
    "                        fold_mae.append(best_val_mae)\n",
    "                    \n",
    "                    mean_mae = np.mean(fold_mae)\n",
    "                    results.append({\n",
    "                        \"model\": model_name,\n",
    "                        \"hidden\": hidden,\n",
    "                        \"dropout\": dropout,\n",
    "                        \"lr\": lr,\n",
    "                        \"pool\": pool,\n",
    "                        \"mean_mae\": mean_mae\n",
    "                    })\n",
    "                    print(f\"{model_name} | hidden={hidden} dropout={dropout} lr={lr} pool={pool} -> MAE={mean_mae:.4f}\")\n",
    "\n",
    "# Save hyperparameter sweep results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"hyperparameter_sweep_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a47dca",
   "metadata": {},
   "source": [
    "## 6. Retrain Folds with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Store retrained fold models\n",
    "fold_models = {name: [] for name in model_names}\n",
    "\n",
    "# Prepare a list to collect summary rows for CSV\n",
    "summary_rows = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    train_dataset = dataset[train_idx.tolist()]\n",
    "    val_dataset   = dataset[val_idx.tolist()]\n",
    "    \n",
    "    train_loader = DisjointLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DisjointLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Collect true targets for validation\n",
    "    val_true = []\n",
    "    for g in val_dataset:\n",
    "        val_true.append(g.y)\n",
    "    val_true = np.vstack(val_true)\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Get best hyperparameters\n",
    "        params = best_params_per_model[model_name]\n",
    "        model = model_factory(model_name, hidden=params[\"hidden\"], dropout=params[\"dropout\"])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(params[\"lr\"]), loss=\"mse\")\n",
    "\n",
    "        # Early stopping\n",
    "        es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "        # Train\n",
    "        model.fit(train_loader.load(), steps_per_epoch=train_loader.steps_per_epoch,\n",
    "                  validation_data=val_loader.load(), validation_steps=val_loader.steps_per_epoch,\n",
    "                  epochs=EPOCHS, verbose=0, callbacks=[es])\n",
    "        \n",
    "        # Save retrained model\n",
    "        fold_models[model_name].append(model)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        val_preds = []\n",
    "        for batch in val_loader:\n",
    "            x, a, i, e, g = batch\n",
    "            y_pred = model([x, a, i, e, g], training=False)\n",
    "            val_preds.append(y_pred.numpy())\n",
    "        val_preds = np.vstack(val_preds)\n",
    "        \n",
    "        # Compute metrics\n",
    "        mae = mean_absolute_error(val_true, val_preds)\n",
    "        mse = mean_squared_error(val_true, val_preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(val_true, val_preds)\n",
    "        \n",
    "        # Append metrics to summary list\n",
    "        summary_rows.append({\n",
    "            \"Fold\": fold+1,\n",
    "            \"Model\": model_name,\n",
    "            \"MAE\": mae,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": rmse,\n",
    "            \"R2\": r2\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold+1}, {model_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}\")\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(\"crossval_summary_reg.csv\", index=False)\n",
    "print(\"\\nCross-validation summary saved to crossval_summary_reg.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf8d4b",
   "metadata": {},
   "source": [
    "## 7. Visualize CV Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load summary CSV\n",
    "summary_df = pd.read_csv(\"crossval_summary_reg.csv\")\n",
    "\n",
    "# Metrics to analyze\n",
    "metrics = [\"MAE\", \"MSE\", \"RMSE\", \"R2\"]\n",
    "\n",
    "# Plot bar charts for each metric across folds\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for model in summary_df[\"Model\"].unique():\n",
    "        model_data = summary_df[summary_df[\"Model\"] == model]\n",
    "        plt.bar(model_data[\"Fold\"] + (0.15 * list(summary_df[\"Model\"].unique()).tolist().index(model)),\n",
    "                model_data[metric], width=0.15, label=model)\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"{metric} across folds\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print mean Â± std for each metric per model\n",
    "for model in summary_df[\"Model\"].unique():\n",
    "    print(f\"\\n=== {model} ===\")\n",
    "    model_data = summary_df[summary_df[\"Model\"] == model]\n",
    "    for metric in metrics:\n",
    "        mean_val = model_data[metric].mean()\n",
    "        std_val = model_data[metric].std()\n",
    "        print(f\"{metric}: {mean_val:.4f} Â± {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df305a",
   "metadata": {},
   "source": [
    "## 8. Do Ensemble Averaging on the test set and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f39c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Import your custom model classes\n",
    "from models import GCNModel, GATModel, GraphSAGEModel, GINModel, MPNNModel\n",
    "\n",
    "# Test set loader (no shuffle, one pass)\n",
    "test_loader = DisjointLoader(test_dataset, batch_size=32, epochs=1, shuffle=False)\n",
    "\n",
    "# Custom objects for loading\n",
    "custom_objects = {\n",
    "    \"GCNModel\": GCNModel,\n",
    "    \"GATModel\": GATModel,\n",
    "    \"GraphSAGEModel\": GraphSAGEModel,\n",
    "    \"GINModel\": GINModel,\n",
    "    \"MPNNModel\": MPNNModel,\n",
    "}\n",
    "\n",
    "# Use your model naming convention\n",
    "model_names = [\"GCN\", \"GAT\", \"GraphSAGE\", \"GIN\", \"MPNN\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n=== Ensemble Averaging for {model_name} ===\")\n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    for fold in range(10):\n",
    "        # Match your checkpoint filenames\n",
    "        model = tf.keras.models.load_model(\n",
    "            f\"checkpoints/{model_name}_fold{fold}.h5\",\n",
    "            custom_objects=custom_objects\n",
    "        )\n",
    "\n",
    "        fold_preds = model.predict(test_loader.load(), verbose=0)\n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "    # Stack and average predictions across folds\n",
    "    all_preds = np.stack(all_preds, axis=0)\n",
    "    ensemble_preds = np.mean(all_preds, axis=0).squeeze()\n",
    "\n",
    "    # Collect true values\n",
    "    y_true = np.concatenate([y for _, y in test_loader], axis=0).squeeze()\n",
    "\n",
    "    # âœ… Metrics\n",
    "    mae = mean_absolute_error(y_true, ensemble_preds)\n",
    "    mse = mean_squared_error(y_true, ensemble_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, ensemble_preds)\n",
    "\n",
    "    results[model_name] = {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "    # ðŸ“ˆ Scatter plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_true, ensemble_preds, alpha=0.6)\n",
    "    plt.plot([y_true.min(), y_true.max()],\n",
    "             [y_true.min(), y_true.max()],\n",
    "             color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Ensemble Predictions\")\n",
    "    plt.title(f\"{model_name}: True vs Ensemble Predictions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save all results\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results.to_csv(\"ensemble_results_testset.csv\", index=True)\n",
    "\n",
    "print(\"\\n=== Ensemble Results Across Models ===\")\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c25a55",
   "metadata": {},
   "source": [
    "## 9. Final model training and test evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Parameters / directories\n",
    "# -----------------------------\n",
    "save_dir = \"final_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# List of model names and corresponding factory functions that build them\n",
    "# Replace these factories with your custom model constructors\n",
    "model_names = [\"GCN\", \"GAT\", \"GraphSAGE\", \"GIN\", \"MPNN\"]\n",
    "model_factories = {\n",
    "    \"GCN\": lambda: GCNModel(n_node_features, n_edge_features, n_globals, n_targets=1),\n",
    "    \"GAT\": lambda: GATModel(n_node_features, n_edge_features, n_globals, n_targets=1),\n",
    "    \"GraphSAGE\": lambda: GraphSAGEModel(n_node_features, n_edge_features, n_globals, n_targets=1),\n",
    "    \"GIN\": lambda: GINModel(n_node_features, n_edge_features, n_globals, n_targets=1),\n",
    "    \"MPNN\": lambda: MPNNModel(n_node_features, n_edge_features, n_globals, n_targets=1),\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Merge all folds into train+val\n",
    "# -----------------------------\n",
    "# Assume you already have your folds stored\n",
    "all_graphs = []\n",
    "for fold_dataset in folds:  # folds is a list of Spektral datasets\n",
    "    all_graphs.extend(fold_dataset.graphs)\n",
    "\n",
    "# Split off 10% for validation\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(len(all_graphs))\n",
    "split = int(0.9 * len(all_graphs))\n",
    "train_graphs = [all_graphs[i] for i in idx[:split]]\n",
    "val_graphs   = [all_graphs[i] for i in idx[split:]]\n",
    "\n",
    "train_dataset = Dataset(train_graphs)\n",
    "val_dataset   = Dataset(val_graphs)\n",
    "\n",
    "train_loader = DisjointLoader(train_dataset, batch_size=32, epochs=1, shuffle=True)\n",
    "val_loader   = DisjointLoader(val_dataset, batch_size=32, epochs=1, shuffle=False)\n",
    "test_loader  = DisjointLoader(test_dataset, batch_size=32, epochs=1, shuffle=False)  # already held-out\n",
    "\n",
    "# -----------------------------\n",
    "# Train final models\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n=== Training final {model_name} model ===\")\n",
    "\n",
    "    # Build model\n",
    "    model = model_factories[model_name]()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10)\n",
    "    ]\n",
    "\n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_loader.load(),\n",
    "        steps_per_epoch=train_loader.steps_per_epoch,\n",
    "        validation_data=val_loader.load(),\n",
    "        validation_steps=val_loader.steps_per_epoch,\n",
    "        epochs=300,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Save final model (one file per model)\n",
    "    save_path = os.path.join(save_dir, f\"{model_name.lower()}_final_model.keras\")\n",
    "    model.save(save_path)\n",
    "    print(f\"âœ… Saved {model_name} model at {save_path}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Evaluate on test set\n",
    "    # -----------------------------\n",
    "    y_true_list, y_pred_list = [], []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        preds = model(x, training=False)\n",
    "        y_true_list.append(y.numpy())\n",
    "        y_pred_list.append(preds.numpy())\n",
    "\n",
    "    y_true = np.vstack(y_true_list).squeeze()\n",
    "    y_pred = np.vstack(y_pred_list).squeeze()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    results.append({\"Model\": model_name, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "    # -----------------------------\n",
    "    # Scatter + regression line\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, label=\"Predictions\")\n",
    "\n",
    "    # Perfect prediction line\n",
    "    min_val, max_val = y_true.min(), y_true.max()\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], \"r--\", label=\"Perfect prediction\")\n",
    "\n",
    "    # Fit regression line\n",
    "    slope, intercept = np.polyfit(y_true, y_pred, 1)\n",
    "    plt.plot(y_true, slope*y_true + intercept, \"b-\", label=f\"Fit: y={slope:.2f}x+{intercept:.2f}\")\n",
    "\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"{model_name} Final Model: True vs Predicted\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f\"{model_name.lower()}_final_scatter.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Save metrics CSV\n",
    "# -----------------------------\n",
    "df_metrics = pd.DataFrame(results)\n",
    "df_metrics.to_csv(os.path.join(save_dir, \"final_metrics_reg.csv\"), index=False)\n",
    "print(\"\\nâœ… Saved final metrics to final_metrics_reg.csv\")\n",
    "print(df_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
